{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ead543",
   "metadata": {},
   "source": [
    "# Prompting with Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa00498",
   "metadata": {},
   "source": [
    "- **Prompt Engineering**: Best practices to *improve* prompts (multishot prompting, structure with XML tags, etc.)\n",
    "- **Prompt Evaluation**: Automated testing to *measure* how well prompts work (test against expected answers, compare different versions, review outputs for errors, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff316fb8",
   "metadata": {},
   "source": [
    "##### Import packages & set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "166a8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10180a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd45d5e",
   "metadata": {},
   "source": [
    "##### Chat functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60841f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_message(messages: list, text: str):\n",
    "    user_mesage = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_mesage)\n",
    "\n",
    "def add_assistant_message(messages: list, text: str):\n",
    "    assistant_mesage = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_mesage)\n",
    "\n",
    "def chat(messages: list,\n",
    "         model: str = \"claude-3-haiku-latest\",\n",
    "         max_tokens: int = 1000,\n",
    "         system = None,\n",
    "         temperature: float = 1.0,\n",
    "         stop_sequences: list=[]\n",
    "    ):\n",
    "    params = {\n",
    "        \"model\": model, \n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences\n",
    "    }\n",
    "    if system:\n",
    "        params['system'] = system\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dae3f6",
   "metadata": {},
   "source": [
    "## 1. Prompt Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb16ea",
   "metadata": {},
   "source": [
    "Run prompt through evaluation pipeline to score it, and iterate con the prompt.\n",
    "\n",
    "*Sample Eval Workflow*: \n",
    "1. Draft a prompt\n",
    "2. Create an Eval Dataset\n",
    "3. Feed through Claude\n",
    "4. Feed through a grader\n",
    "5. Change prompt and repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a6fa2",
   "metadata": {},
   "source": [
    "**Exercise**: Write a prompt that will assist users in writing Python code, JSON config, or Regular expressions focused on AWS-specific use cases.\n",
    "- **Input**: User's request\n",
    "- **Output**: Python, JSON, or regular expression wo any explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d29585",
   "metadata": {},
   "source": [
    "#### 1.1 Generate Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862a3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate an evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects, each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "{\n",
    "    \"task\": \"Description of task\",\n",
    "    \"format\": \"json\" or \"python\" or \"regex\",\n",
    "    \"solution_criteria\": \"Key criteria for evaluating the solution\"\n",
    "},\n",
    "...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a single regex\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"], model = \"claude-3-haiku-20240307\")\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea3faa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task': 'Create an AWS Lambda function to handle a simple API Gateway request that returns a JSON response.',\n",
       "  'format': 'python',\n",
       "  'solution_criteria': 'The solution should include a Python function that takes an event object as input, processes the request, and returns a JSON response.'},\n",
       " {'task': 'Define a JSON structure to represent an AWS EC2 instance configuration, including instance type, image ID, key pair, and security group.',\n",
       "  'format': 'json',\n",
       "  'solution_criteria': 'The solution should include a valid JSON object that accurately represents the required EC2 instance configuration.'},\n",
       " {'task': 'Write a regular expression to validate an AWS IAM user name, which must be between 1 and 64 characters long and can only contain alphanumeric characters and the following special characters: plus (+), equal (=), comma (,), period (.), at (@), and hyphen (-).',\n",
       "  'format': 'regex',\n",
       "  'solution_criteria': 'The solution should include a regular expression that accurately validates the given AWS IAM user name requirements.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = generate_dataset()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfc5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "with open(\"02_PromptEvaluation_outputs/dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a93fa",
   "metadata": {},
   "source": [
    "#### 1.2 Running the eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396e3ef",
   "metadata": {},
   "source": [
    "**Graders**:\n",
    "\n",
    "| Grader | Description | Uses | \n",
    "| -- | -- | -- |\n",
    "| **Code** | Programatically evaluate the result. | - Check output length <br> - Verify words in output  <br> - Syntax validation  <br> - Readability scores |\n",
    "| **Model** | Ask a model to assign a score to the output, or compare 2 versions | - Response quality <br> - Instruction following <br> - Completeness <br> - Helpfulness <br> - Safety |\n",
    "| **Human** | Ask a human to assign a score to the output, or compare 2 versions | - General response quality <br> - Comprengeniveness <br> - Depth <br> - Conciseness <br> - Relevance |\n",
    "\n",
    "\n",
    "**Evaluation Criteria for Exercise**:\n",
    "- Format: Should return only Python, JSON, or Regex, no explanation.\n",
    "- Valid Syntax: Produced Python, JSON and Regex should have correct syntax.\n",
    "- Task Following: Response should directly and clearly address the user's task; generated code should be accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a9f5c",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5713ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a code based grader for *Format and Syntax*\n",
    "\n",
    "# JSON\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "# Python\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "# Regex\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "# Code grader\n",
    "def code_grader(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    match format:\n",
    "        case \"Python\" | \"python\":\n",
    "            score = validate_python(response)\n",
    "        case \"Json\" | \"json\" | \"JSON\":\n",
    "            score = validate_json(response)\n",
    "        case \"Regex\" | \"regex\":\n",
    "            score = validate_regex(response)\n",
    "    return float(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "22e98864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model grader for *Task Following\n",
    "def model_grader(test_case, output):\n",
    "    # Create evaluation prompt\n",
    "    eval_prompt = f\"\"\"\n",
    "    You are an expert AWS code reviewer. Evaluate this AI-generated solution, consider how much it aligns to the solution criteria provided.\n",
    "    \n",
    "    ORIGINAL TASK:\n",
    "    <task>\n",
    "    {test_case['task']}\n",
    "    </task>\n",
    "\n",
    "    SOLUTION TO EVALUATE:\n",
    "    <solution>\n",
    "    {output}\n",
    "    </solution>\n",
    "\n",
    "    SOLUTION_CRITERIA:\n",
    "    <solution_criteria>\n",
    "    {test_case['solution_criteria']}\n",
    "    </solution_criteria>\n",
    "\n",
    "    OUTPUT FORMAT\n",
    "    Provide your evaluation as a structured JSON object with:\n",
    "    - \"strengths\": An array of 1-3 key strengths\n",
    "    - \"weaknesses\": An array of 1-3 key areas for improvement  \n",
    "    - \"reasoning\": A concise explanation of your assessment\n",
    "    - \"score\": A number between 1-10\n",
    "\n",
    "    Respond with JSON. Keep the response concise and direct.\n",
    "    Sample response:\n",
    "    {{\n",
    "      \"strengths\": string[],\n",
    "      \"weaknesses\": string[],\n",
    "      \"reasoning\": string,\n",
    "      \"score\": number,\n",
    "\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae54df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(prompt, test_case):\n",
    "    \"\"\"Merges the prompt and test case input\"\"\"\n",
    "    prompt_merged = prompt(test_case)\n",
    "    return prompt_merged\n",
    "\n",
    "def run_prompt(prompt, test_case):\n",
    "    \"\"\" returns the result\"\"\"\n",
    "    prompt = create_prompt(prompt, test_case)\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output\n",
    "\n",
    "def score_output(prompt, test_case):\n",
    "    \"\"\"Calls run_prompt and grades the result\"\"\"\n",
    "    # Get response \n",
    "    output = run_prompt(prompt, test_case)\n",
    "\n",
    "    # Grading\n",
    "    model_score = model_grader(test_case, output)\n",
    "    syntax_score = code_grader(output, test_case)\n",
    "    score = (2 * syntax_score/3 + model_score['score']/3)\n",
    "\n",
    "    result = {\n",
    "        \"test_case\": test_case,\n",
    "        \"output\": output,\n",
    "        \"strengths\": model_score['strengths'],\n",
    "        \"weaknesses\": model_score['weaknesses'],\n",
    "        \"reasoning\": model_score['reasoning'],\n",
    "        \"model_score\": model_score['score'],\n",
    "        \"syntax_score\": syntax_score,\n",
    "        \"final_score\": score\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def run_full_eval(prompt, dataset):\n",
    "    \"\"\"Loads dataset and calls score_output with each case\"\"\"\n",
    "    results = []\n",
    "    for test_case in dataset:\n",
    "        result = score_output(prompt, test_case)\n",
    "        results.append(result)\n",
    "    \n",
    "    average_score = mean([result[\"final_score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "    \n",
    "    return {\"prompt\": create_prompt(prompt, \"test_case\"), \"results\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08cd56e",
   "metadata": {},
   "source": [
    "##### Prompt V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c1c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 9.333333333333334\n"
     ]
    }
   ],
   "source": [
    "prompt_v1 = lambda x: f\"\"\"Provide a solution to the following task:\n",
    "{x}\n",
    "\"\"\"\n",
    "\n",
    "results = run_full_eval(prompt = prompt_v1, dataset = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_prompt_v1.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c734820",
   "metadata": {},
   "source": [
    "##### Prompt V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b149ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 10.0\n"
     ]
    }
   ],
   "source": [
    "prompt_v2 = lambda x: f\"\"\"Provide a solution to the following task:\n",
    "{x}\n",
    "\n",
    "* Respond only with Python, JSON, or plain Regex.\n",
    "* Do not add any comments or commentary or explanation.\n",
    "\"\"\"\n",
    "\n",
    "results = run_full_eval(prompt = prompt_v2, dataset = dataset)\n",
    "\n",
    "with open(\"eval_prompt_v2.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
