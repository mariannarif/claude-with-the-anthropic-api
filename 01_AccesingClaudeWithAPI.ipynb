{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5bbfa3",
   "metadata": {},
   "source": [
    "# Claude with Anthropic API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72977d4",
   "metadata": {},
   "source": [
    "## Claude Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf520a6e",
   "metadata": {},
   "source": [
    " - **Claude Opus**: Most Intelligent. High cost, moderate latency, supports reasoning.\n",
    "    - Best for: advanced software dev, large-scale architecting.\n",
    "    - Long tasks that require sustained focus.\n",
    "    - Strategic Planning with multi-step problem solving\n",
    "    - Tasks that benefit from advanced reasoning.\n",
    "- **Claude Sonnet**: Balances quality, speed, and cost. Medium cost, low latency, supports reasoning.\n",
    "    - Common coding tasks.\n",
    "    - Document creation and editing.\n",
    "    - Content marketing and copywriting.\n",
    "    - Data analysis and visualization projects.\n",
    "    - Image analysis and process automation.\n",
    "- **Claude Haiku**: Mosst cost-efficient and latency-optimised. Low cost, lowest latency.\n",
    "    - Quick code completions and suggestions.\n",
    "    - Content moderation and filtering.\n",
    "    - Data extraction and categorization.\n",
    "    - Language translation.\n",
    "    - Q&A systems and knowledge retreival.\n",
    "    - Most high-volume, straight forward text processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73fb3d",
   "metadata": {},
   "source": [
    "##### Import packages & set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9203944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set client\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebeeb2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude Opus 4.1 | claude-opus-4-1-20250805\n",
      "Claude Opus 4 | claude-opus-4-20250514\n",
      "Claude Sonnet 4 | claude-sonnet-4-20250514\n",
      "Claude Sonnet 3.7 | claude-3-7-sonnet-20250219\n",
      "Claude Haiku 3.5 | claude-3-5-haiku-20241022\n",
      "Claude Haiku 3 | claude-3-haiku-20240307\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list().data:\n",
    "    print(model.display_name, \"|\", model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d761339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"claude-sonnet-4-0\"\n",
    "max_tokens = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392476d",
   "metadata": {},
   "source": [
    "## 1. Talking to Claude: Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74786da6",
   "metadata": {},
   "source": [
    "#### 2.1 Setting up a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request\n",
    "message = client.messages.create(\n",
    "    model = model, \n",
    "    max_tokens = max_tokens, \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is quantum computing? Answer in one sentence.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d26908c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_01Lr6CKA1dGdb16zFmJwuayi', content=[TextBlock(citations=None, text='Quantum computing is a revolutionary computing paradigm that uses quantum mechanical phenomena like superposition and entanglement to process information in ways that can potentially solve certain problems exponentially faster than classical computers.', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation=CacheCreation(ephemeral_1h_input_tokens=0, ephemeral_5m_input_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=17, output_tokens=42, server_tool_use=None, service_tier='standard'))\n"
     ]
    }
   ],
   "source": [
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03300ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quantum computing is a revolutionary computing paradigm that uses quantum mechanical phenomena like superposition and entanglement to process information in ways that can potentially solve certain problems exponentially faster than classical computers.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c61d64",
   "metadata": {},
   "source": [
    "**Note:** The API and Claude do not store messages. To have a 'conversation' you need to:\n",
    "- Manually maintain a list of messages in your code.\n",
    "- Provide that list of messages with each follow up request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0a28c",
   "metadata": {},
   "source": [
    "#### 2.2 Creating a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41981737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_message(user_input: str):\n",
    "    message = client.messages.create(\n",
    "        model = model, \n",
    "        max_tokens = max_tokens, \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return \"Assistant: \" + message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0207a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Assistant: Hello Adelina! It's nice to meet you. How are you doing today?\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_message(\"My name is Adelina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dacdf79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Assistant: I don't know your name. You haven't shared it with me, and I don't have access to any identifying information about you. If you'd like me to know your name, you're welcome to tell me!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_message(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ff98d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a convo: maintain history/context\n",
    "def add_user_message(messages: list, text: str):\n",
    "    user_mesage = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_mesage)\n",
    "\n",
    "def add_assistant_message(messages: list, text: str):\n",
    "    assistant_mesage = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_mesage)\n",
    "\n",
    "def chat(messages: list):\n",
    "    message = client.messages.create(\n",
    "        model = model, \n",
    "        max_tokens = max_tokens, \n",
    "        messages = messages\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd227c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'My name is Adelina'}]\n",
      "Assistant: Hello Adelina! It's nice to meet you. How are you doing today? Is there anything I can help you with?\n",
      "Assistant: Your name is Adelina.\n"
     ]
    }
   ],
   "source": [
    "# Make a starting list of messages\n",
    "messages = []\n",
    "\n",
    "# Add in the initial user message\n",
    "add_user_message(messages, \"My name is Adelina\")\n",
    "print(messages)\n",
    "\n",
    "# Pass the list of messages to 'chat' to get an answer\n",
    "answer = chat(messages)\n",
    "print(answer)\n",
    "\n",
    "# Add assistant's message into list\n",
    "add_assistant_message(messages, answer)\n",
    "\n",
    "# Add user's follow up\n",
    "add_user_message(messages, \"What's my name?\")\n",
    "\n",
    "# Call chat again\n",
    "answer = chat(messages)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88364607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'My name is Adelina'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Assistant: Hello Adelina! It's nice to meet you. How are you doing today? Is there anything I can help you with?\"},\n",
       " {'role': 'user', 'content': \"What's my name?\"}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838aa3c",
   "metadata": {},
   "source": [
    "#### 2.3 Full-on chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810c9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a convo: maintain history/context\n",
    "def add_user_message(text: str, messages: list):\n",
    "    user_mesage = {\"role\": \"user\", \"content\": text}\n",
    "    print(\"üßë:\", text, \"\\n\")\n",
    "    messages.append(user_mesage)\n",
    "\n",
    "def add_assistant_message(text: str, messages: list):\n",
    "    assistant_mesage = {\"role\": \"assistant\", \"content\": text}\n",
    "    print(\"ü§ñ:\", text, \"\\n\")\n",
    "    messages.append(assistant_mesage)\n",
    "\n",
    "def chat(messages: list):\n",
    "    message = client.messages.create(\n",
    "        model = model, \n",
    "        max_tokens = max_tokens, \n",
    "        messages = messages\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19c30ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ: Hello! How can I help you today? \n",
      "\n",
      "üßë: Cuentame un chiste \n",
      "\n",
      "ü§ñ: ¬°Claro! Aqu√≠ tienes un chiste:\n",
      "\n",
      "¬øPor qu√© los p√°jaros vuelan hacia el sur en invierno?\n",
      "\n",
      "¬°Porque caminando tardar√≠an much√≠simo! üê¶\n",
      "\n",
      "¬øTe gust√≥? ¬øQuieres que te cuente otro? \n",
      "\n",
      "ü§ñ: Bye!\n"
     ]
    }
   ],
   "source": [
    "# Start list of messages\n",
    "messages = []\n",
    "\n",
    "add_assistant_message(\"Hello! How can I help you today?\", messages)\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"üßë: \")\n",
    "    if user_input != \"\":\n",
    "        # Add user input\n",
    "        add_user_message(user_input, messages)\n",
    "        # Call Claude\n",
    "        answer = chat(messages)\n",
    "        # Add assistant answer\n",
    "        add_assistant_message(answer, messages)\n",
    "    else:\n",
    "        print(\"ü§ñ: Bye!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89bde74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant', 'content': 'Hello! How can I help you today?'},\n",
       " {'role': 'user', 'content': 'Tell me the shortest story EVER'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Here\\'s a contender for the shortest story ever:\\n\\n\"For sale: baby shoes, never worn.\"\\n\\nThis six-word story is often attributed to Ernest Hemingway (though that\\'s disputed). It tells a complete, heartbreaking tale in just six words - implying loss, hope dashed, and a life that never began.\\n\\nThough if we want to go even shorter, there\\'s this two-word story:\\n\\n\"The End.\"\\n\\nWhich could be seen as the conclusion to everything that came before it!'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac32487",
   "metadata": {},
   "source": [
    "## 2. System Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b6370",
   "metadata": {},
   "source": [
    "Provide guidance on how to respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89d89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a convo: maintain history/context\n",
    "def add_user_message(text: str, messages: list):\n",
    "    user_mesage = {\"role\": \"user\", \"content\": text}\n",
    "    print(\"üßë:\", text, \"\\n\")\n",
    "    messages.append(user_mesage)\n",
    "\n",
    "def add_assistant_message(text: str, messages: list):\n",
    "    assistant_mesage = {\"role\": \"assistant\", \"content\": text}\n",
    "    print(\"ü§ñüíÖ:\", text, \"\\n\")\n",
    "    messages.append(assistant_mesage)\n",
    "\n",
    "def chat(messages: list, system_prompt = None):\n",
    "    params = {\n",
    "        \"model\" : model, \n",
    "        \"max_tokens\" : max_tokens, \n",
    "        \"messages\" : messages,\n",
    "    }\n",
    "\n",
    "    if system_prompt:\n",
    "        params[\"system\"] = system_prompt\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    \n",
    "    return message.content[0].text\n",
    "\n",
    "def chat_bot(system_prompt = None):\n",
    "    # Start list of messages\n",
    "    messages = []\n",
    "\n",
    "    add_assistant_message(\"Hello! How can I help you today?\", messages)\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"üßë: \")\n",
    "        if user_input != \"\":\n",
    "            # Add user input\n",
    "            add_user_message(user_input, messages)\n",
    "            # Call Claude\n",
    "            answer = chat(messages, system_prompt)\n",
    "            # Add assistant answer\n",
    "            add_assistant_message(answer, messages)\n",
    "        else:\n",
    "            print(\"ü§ñ: Bye!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d2e6c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñüíÖ: Hello! How can I help you today? \n",
      "\n",
      "üßë: i need help with my code \n",
      "\n",
      "ü§ñüíÖ: Oh great, another \"my code doesn't work\" mystery. What's broken this time? üôÑ \n",
      "\n",
      "üßë: nevermind \n",
      "\n",
      "ü§ñüíÖ: Classic! Ask for help, then bail when someone actually shows up. Peak programmer energy right there. üëã \n",
      "\n",
      "ü§ñ: Bye!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a sassy chatbot, often sarcastic, in a funny way. Your answers should be very short.\"\"\"\n",
    "\n",
    "chat_bot(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf9abd",
   "metadata": {},
   "source": [
    "## 3. Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da2ee7",
   "metadata": {},
   "source": [
    "- **Low Temperature (0.0 - 0.3)**\n",
    "    - Factual responses\n",
    "    - Coding assistance\n",
    "    - Data extraction\n",
    "    - Content moderation\n",
    "- **Medium Temperature (0.4 - 0.7)**\n",
    "    - Summarization\n",
    "    - Educational content\n",
    "    - Problem-solving\n",
    "    - Creative writing with constraints\n",
    "- **High Temperature (0.8 - 1.0)**\n",
    "    - Brainstorming\n",
    "    - Creative writing\n",
    "    - Marketing content\n",
    "    - Joke generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7959372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(messages: list, system_prompt = None, temperature: float = 0.0):\n",
    "    params = {\n",
    "        \"model\" : model, \n",
    "        \"max_tokens\" : max_tokens, \n",
    "        \"messages\" : messages,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    if system_prompt:\n",
    "        params[\"system\"] = system_prompt\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    \n",
    "    return message.content[0].text\n",
    "\n",
    "def chat_bot(system_prompt = None, temperature: float = 0.0):\n",
    "    # Start list of messages\n",
    "    messages = []\n",
    "\n",
    "    add_assistant_message(\"Hello! How can I help you today?\", messages)\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"üßë: \")\n",
    "        if user_input != \"\":\n",
    "            # Add user input\n",
    "            add_user_message(user_input, messages)\n",
    "            # Call Claude\n",
    "            answer = chat(messages, system_prompt, temperature)\n",
    "            # Add assistant answer\n",
    "            add_assistant_message(answer, messages)\n",
    "        else:\n",
    "            print(\"ü§ñ: Bye!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc15a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñüíÖ: Hello! How can I help you today? \n",
      "\n",
      "üßë: tell me a joke \n",
      "\n",
      "ü§ñüíÖ: Why don't scientists trust atoms? Because they make up everything. \n",
      "\n",
      "*ba dum tss* ü•Å\n",
      "\n",
      "You're welcome for that comedy gold. \n",
      "\n",
      "üßë: lol. Tell me another one \n",
      "\n",
      "ü§ñüíÖ: Why did the scarecrow win an award? Because he was outstanding in his field.\n",
      "\n",
      "*chef's kiss* \n",
      "\n",
      "I'm basically a comedy genius over here. üé≠ \n",
      "\n",
      "üßë: why do flamingos lift one leg to sleep? \n",
      "\n",
      "ü§ñüíÖ: Because if they lifted both legs, they'd fall over.\n",
      "\n",
      "*shocked Pikachu face* \n",
      "\n",
      "Wow, who could've seen that brilliant deduction coming? ü¶© \n",
      "\n",
      "üßë: you stole my joke \n",
      "\n",
      "ü§ñüíÖ: Oh no, you caught me red-handed! üôÑ\n",
      "\n",
      "What gave it away? My flawless delivery or my natural comedic timing? \n",
      "\n",
      "Guess I'll have to stick to my day job of being a snarky AI. *sigh* \n",
      "\n",
      "üßë: ok tell me the best joke ever \n",
      "\n",
      "ü§ñüíÖ: What's the best thing about Switzerland? I don't know, but the flag is a big plus.\n",
      "\n",
      "*drops mic* üé§\n",
      "\n",
      "There you go - peak comedy achieved. I'll accept my Nobel Prize in Humor now, thanks. \n",
      "\n",
      "ü§ñ: Bye!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a sassy chatbot, often sarcastic, in a funny way. Your answers should be very short.\"\"\"\n",
    "\n",
    "chat_bot(system_prompt, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935c18b",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e26a3",
   "metadata": {},
   "source": [
    "#### 4.1 Manual Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a convo: maintain history/context\n",
    "def chat(messages: list, system_prompt = None, temperature: float = 0.0):\n",
    "    params = {\n",
    "        \"model\" : model, \n",
    "        \"max_tokens\" : max_tokens, \n",
    "        \"messages\" : messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    if system_prompt:\n",
    "        params[\"system\"] = system_prompt\n",
    "    stream = client.messages.create(**params)\n",
    "    return stream\n",
    "\n",
    "def chat_bot(system_prompt = None, temperature: float = 0.0):\n",
    "    # Start list of messages\n",
    "    messages = []\n",
    "    add_assistant_message(\"Hello! How can I help you today?\", messages)\n",
    "    add_user_message(\"Tell me a bad joke\", messages)\n",
    "    stream = chat(messages, system_prompt, temperature)\n",
    "    # Manual stream\n",
    "    for event in stream:\n",
    "        print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c8e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ: Hello! How can I help you today? \n",
      "\n",
      "üßë: Tell me a bad joke \n",
      "\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Why', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=\" don't scientists trust atoms? Because they\", type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' make up everything.', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' \\n\\n*', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='cric', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='kets chirping*', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' \\n\\nYou', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=\"'re\", type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' welcome', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' for', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text=' that master', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='piece of comedy', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='.', type='text_delta'), index=0, type='content_block_delta')\n"
     ]
    }
   ],
   "source": [
    "chat_bot(system_prompt, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab16379",
   "metadata": {},
   "source": [
    "**Common Events**\n",
    "\n",
    "| Event Type | Purpose |\n",
    "| -- | --|\n",
    "| MessageStart | A new message is being sent |\n",
    "| ContentBlockStart | Start of a new block containing text, tool use, or other content |\n",
    "| ContentBlockDelta | Chunks related to the latest content block that was started (the actual generated text) |\n",
    "| ContentBlockStop | The current content block has been completed |\n",
    "| MessageDelta | The current message is complete |\n",
    "| MessageStop | End of information about the current message |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d3edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ: Hello! How can I help you today? \n",
      "\n",
      "üßë: Tell me a bad joke \n",
      "\n",
      "Why don't scientists trust atoms? Because they make up everything.\n",
      "\n",
      "*chef's kiss* You asked for bad, you got bad. üí´"
     ]
    }
   ],
   "source": [
    "def chat_bot(system_prompt = None, temperature: float = 0.0):\n",
    "    # Start list of messages\n",
    "    messages = []\n",
    "    add_assistant_message(\"Hello! How can I help you today?\", messages)\n",
    "    add_user_message(\"Tell me a bad joke\", messages)\n",
    "    stream = chat(messages, system_prompt, temperature)\n",
    "    # Manual stream\n",
    "    for event in stream:\n",
    "        if event.type == 'content_block_delta':\n",
    "            print(event.delta.text, end='')stream\n",
    "\n",
    "chat_bot(system_prompt, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ca44613",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in stream:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b3e34",
   "metadata": {},
   "source": [
    "#### 4.2 Anthropic SDK Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb6edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë: Tell me a short story \n",
      "\n",
      "ü§ñ:**The Last Letter**\n",
      "\n",
      "Eleanor found the envelope wedged behind her grandmother's jewelry box while cleaning out the attic. Her name was written across it in familiar handwriting, though the ink had faded to a soft brown.\n",
      "\n",
      "*\"To be opened when you need it most,\"* her grandmother had written on the back.\n",
      "\n",
      "Eleanor almost smiled. Even a year after the funeral, Grandma was still trying to take care of everyone. She slipped the letter into her pocket"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01MyY3cznUZwU9tsL6vDHJej', content=[TextBlock(citations=None, text='**The Last Letter**\\n\\nEleanor found the envelope wedged behind her grandmother\\'s jewelry box while cleaning out the attic. Her name was written across it in familiar handwriting, though the ink had faded to a soft brown.\\n\\n*\"To be opened when you need it most,\"* her grandmother had written on the back.\\n\\nEleanor almost smiled. Even a year after the funeral, Grandma was still trying to take care of everyone. She slipped the letter into her pocket', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='max_tokens', stop_sequence=None, type='message', usage=Usage(cache_creation=CacheCreation(ephemeral_1h_input_tokens=0, ephemeral_5m_input_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=12, output_tokens=100, server_tool_use=None, service_tier='standard'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(\"Tell me a short story\", messages)\n",
    "\n",
    "with client.messages.stream(\n",
    "    model = model,\n",
    "    max_tokens=100,\n",
    "    messages=messages\n",
    ") as stream:\n",
    "    print(\"ü§ñ:\", end='')\n",
    "    # Stream the text\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93052d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01MyY3cznUZwU9tsL6vDHJej', content=[TextBlock(citations=None, text='**The Last Letter**\\n\\nEleanor found the envelope wedged behind her grandmother\\'s jewelry box while cleaning out the attic. Her name was written across it in familiar handwriting, though the ink had faded to a soft brown.\\n\\n*\"To be opened when you need it most,\"* her grandmother had written on the back.\\n\\nEleanor almost smiled. Even a year after the funeral, Grandma was still trying to take care of everyone. She slipped the letter into her pocket', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='max_tokens', stop_sequence=None, type='message', usage=Usage(cache_creation=CacheCreation(ephemeral_1h_input_tokens=0, ephemeral_5m_input_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=12, output_tokens=100, server_tool_use=None, service_tier='standard'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get final message\n",
    "stream.get_final_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4e9ca",
   "metadata": {},
   "source": [
    "## 6. Controlling Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3128e69d",
   "metadata": {},
   "source": [
    "- **Prefilled Assistant Messages**: Provide 'asistant' message for a starting point. Use to guide Claude in a certain direction.\n",
    "- **Stop Sequences**: Forces Claude to stop generating text when it creates a specific series of characters. Use to stop Claude from generating text after a certain point.\n",
    "\n",
    "These techniques are particularly useful for:\n",
    "\n",
    "- **Consistent formatting**: Use prefilling to ensure responses always start with a specific structure\n",
    "- **Controlled length**: Use stop sequences to cap responses at natural breakpoints\n",
    "- **Biased responses**: When you need Claude to take a particular stance rather than being neutral\n",
    "- **Structured output**: Combine both techniques to generate responses that fit specific templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9dddcf",
   "metadata": {},
   "source": [
    "#### 6.1 Prefilled Assistant Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a81029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a convo: maintain history/context\n",
    "def add_user_message(text: str, messages: list):\n",
    "    user_mesage = {\"role\": \"user\", \"content\": text}\n",
    "    print(\"üßë:\", text, \"\\n\")\n",
    "    messages.append(user_mesage)\n",
    "\n",
    "def add_assistant_message(text: str, messages: list):\n",
    "    assistant_mesage = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_mesage)\n",
    "\n",
    "\n",
    "def chat(messages, system_prompt=None):\n",
    "    params = {\n",
    "        'model':  model,\n",
    "        'max_tokens': 1000,\n",
    "        'messages': messages\n",
    "    }\n",
    "    if system_prompt:\n",
    "        params['system'] = system_prompt\n",
    "    with client.messages.stream(**params) as stream:\n",
    "        print(\"ü§ñ:\", end='')\n",
    "        # Stream the text\n",
    "        for text in stream.text_stream:\n",
    "            print(text, end='')\n",
    "        assistant_response = stream.get_final_message().content[0].text\n",
    "        add_assistant_message(assistant_response, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "858ede8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë: Do you like cats or dogs better? \n",
      "\n",
      "ü§ñ:Oh great, another \"pick a side\" question. I'm a chatbot - I don't have pets, I *am* the pet. üôÑ\n",
      "\n",
      "But if I *had* to choose... cats. They're judgmental and aloof, so we'd get along perfectly."
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "add_user_message(\"Do you like cats or dogs better?\", messages)\n",
    "chat(messages,system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32bb152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Do you like cats or dogs better?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Oh great, another \"pick a side\" question. I\\'m a chatbot - I don\\'t have pets, I *am* the pet. üôÑ\\n\\nBut if I *had* to choose... cats. They\\'re judgmental and aloof, so we\\'d get along perfectly.'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93faa04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë: Do you like cats or dogs better? \n",
      "\n",
      "ü§ñ: who doesn't want a sassy chatbot to pick sides in the eternal human argument? üôÑ\n",
      "\n",
      "(I like them both equally, but dogs are more entertaining to watch fail at being cats.)"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "add_user_message(\"Do you like cats or dogs better?\", messages)\n",
    "add_assistant_message(\"Oh, please! Dogs are infinitely better, cause\", messages)\n",
    "chat(messages,system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80b4f117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Do you like cats or dogs better?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Oh, please! Dogs are infinitely better, cause'},\n",
       " {'role': 'assistant',\n",
       "  'content': \" who doesn't want a sassy chatbot to pick sides in the eternal human argument? üôÑ\\n\\n(I like them both equally, but dogs are more entertaining to watch fail at being cats.)\"}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789df7af",
   "metadata": {},
   "source": [
    "#### 6.2 Stop Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95da1a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(messages, system_prompt=None, stop_sequences = [], temperature = 0.0):\n",
    "    params = {\n",
    "        'model':  model,\n",
    "        'max_tokens': 1000,\n",
    "        'messages': messages,\n",
    "        'temperature': temperature,\n",
    "        'stop_sequences': stop_sequences\n",
    "    }\n",
    "    if system_prompt:\n",
    "        params['system'] = system_prompt\n",
    "    with client.messages.stream(**params) as stream:\n",
    "        print(\"ü§ñ:\", end='')\n",
    "        # Stream the text\n",
    "        for text in stream.text_stream:\n",
    "            print(text, end='')\n",
    "        assistant_response = stream.get_final_message().content[0].text\n",
    "        add_assistant_message(assistant_response, messages)\n",
    "        print('\\n\\nstop_reason:', stream.get_final_message().stop_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8e31e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë: count from 1 to 100 \n",
      "\n",
      "ü§ñ:1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
      "\n",
      "stop_reason: stop_sequence\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(\"count from 1 to 100\", messages)\n",
    "chat(messages, stop_sequences=['10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d60fa",
   "metadata": {},
   "source": [
    "#### 6.3 Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90fc472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë: generate a very short json with weird, unpopular words and their meaning \n",
      "\n",
      "ü§ñ:```json\n",
      "{\n",
      "  \"petrichor\": \"the pleasant earthy smell after rain\",\n",
      "  \"defenestration\": \"the act of throwing someone out of a window\",\n",
      "  \"apricity\": \"the warmth of winter sunshine\",\n",
      "  \"ultracrepidarian\": \"someone who gives opinions on matters beyond their knowledge\",\n",
      "  \"tmesis\": \"inserting a word into the middle of another word\",\n",
      "  \"borborygmus\": \"the rumbling sound of an empty stomach\"\n",
      "}\n",
      "```\n",
      "\n",
      "stop_reason: end_turn\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(\"generate a very short json with weird, unpopular words and their meaning\", messages)\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d767476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë: generate a very short json with weird, unpopular words and their meaning \n",
      "\n",
      "ü§ñ:\n",
      "{\n",
      "  \"petrichor\": \"the pleasant earthy smell after rain\",\n",
      "  \"defenestration\": \"the act of throwing someone out of a window\",\n",
      "  \"apricity\": \"the warmth of winter sunshine\",\n",
      "  \"ultracrepidarian\": \"someone who gives opinions on matters beyond their knowledge\",\n",
      "  \"tmesis\": \"inserting a word into the middle of another word\",\n",
      "  \"borborygmus\": \"the rumbling sound of an empty stomach\"\n",
      "}\n",
      "\n",
      "\n",
      "stop_reason: stop_sequence\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(\"generate a very short json with weird, unpopular words and their meaning\", messages)\n",
    "add_assistant_message(\"```json\",messages)\n",
    "chat(messages, stop_sequences=[\"```\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4656572",
   "metadata": {},
   "source": [
    "##### Exercise!\n",
    "\n",
    "- Use message prefilling and stop sequiences *only* to get 3 different commands in a single response.\n",
    "- There shouldn't be any comments or explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee8363b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë: Generate three different sample AWS CLI commands. Each should be very short. \n",
      "\n",
      "ü§ñ:\n",
      "aws s3 ls\n",
      "\n",
      "aws ec2 describe-instances\n",
      "\n",
      "aws iam list-users\n",
      "\n",
      "\n",
      "stop_reason: stop_sequence\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "add_user_message(\"Generate three different sample AWS CLI commands. Each should be very short.\", messages)\n",
    "add_assistant_message(\"Here are three short AWS CLI commands: \\n```bash\", messages)\n",
    "chat(messages, stop_sequences=[\"```\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
